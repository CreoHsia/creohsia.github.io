{
  "hash": "cd23d933daa1eee2f3be971c684620ba",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Consistency of MLE\"\nauthor: \"Creo Hsia\"\ndate: \"2024-10-14\"\ndescription: \"A teplate\"\ncategories: [Template]\nformat: \n  html:\n    code-fold: true\n    code-copy: true  # 代码块显示复制按钮\n    code-overflow: 'scroll'\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    editor: visual\nknitr:\n  opts_chunk: \n    fig.path: \"image/\"\n    collapse: true\n    comment: \"#>|\"\n---\n\n\n\n------------------------------------------------------------------------\n\n\nThe first property is that, as the number of observations $n$ becomes large, the estimate $\\hat{\\theta}$ converges to the true value $\\theta_{\\star}$.\n\n## Theorem\nAssume:\n\n$$\n\\begin{aligned}\n\\textnormal{(Maximizer:)}&\\quad \\widehat{\\theta}=\\underset{\\theta \\in \\Theta}{\\arg \\max } \\widehat{M}(\\theta), \\\\\n\\textnormal{(Uniform LLN):} &\\quad \\sup _{\\theta \\in \\Theta}|\\widehat{M}(\\theta)-M(\\theta)| \\xrightarrow{\\operatorname{pr}} 0, \\\\\n\\textnormal{(Well-separation):} &\\quad \\forall \\varepsilon>0, \\quad \\sup _{\\theta:\\left|\\theta-\\theta_{\\star}\\right|>\\varepsilon} M(\\theta)<M\\left(\\theta_{\\star}\\right).\n\\end{aligned}\n$$\n\nThen $\\widehat{\\theta} \\xrightarrow{\\mathrm{pr}} \\theta_{\\star}$.\n\n**Intuition**: Since two functions $\\widehat{M}(\\theta)$ and $M(\\theta)$ are getting closer, the points of maximum should also get closer, implying that $\\hat{\\theta} \\rightarrow \\theta_0$.\n\n### Proof\n\nFirst, we examine the difference in the true Kullback-Leibler (KL) divergence between the true parameter $\\theta_{\\star}$ and the maximum likelihood estimator (MLE) $\\hat{\\theta}$:\n$$\n\\begin{aligned}\n0 \\leq M(\\theta_{\\star}) - M(\\hat{\\theta}) &= \\left\\{ M(\\theta_{\\star}) - \\widehat{M}(\\hat{\\theta}) \\right\\} + \\left\\{ \\widehat{M}(\\hat{\\theta}) - M(\\hat{\\theta}) \\right\\} \\quad \\text{Add and subtract } \\widehat{M}(\\hat{\\theta}) \\\\\n&\\leq \\left\\{ M(\\theta_{\\star}) - \\widehat{M}(\\theta_{\\star}) \\right\\} + \\left\\{ \\widehat{M}(\\hat{\\theta}) - M(\\hat{\\theta}) \\right\\} \\quad \\hat{\\theta} \\text{ maximizes } \\widehat{M}(\\theta) \\\\\n&\\leq 2 \\sup_{\\theta \\in \\Theta} \\left| \\widehat{M}(\\theta) - M(\\theta) \\right| \\xrightarrow{\\text{pr}} 0. \\quad \\text{Uniform convergence}\n\\end{aligned}\n$$\nThis indicates that the difference converges to zero in probability. Given the strong identifiability of $\\theta_{\\star}$, there exists a $\\delta > 0$ such that:\n$$\n\\left|\\theta-\\theta_{\\star}\\right| \\geq \\varepsilon \\quad \\Rightarrow \\quad M(\\theta)<M\\left(\\theta_{\\star}\\right)-\\delta \n$$\nNote that if $A \\Rightarrow B$, then $\\mathrm{P}(A) \\leq \\mathrm{P}(B)$. So, putting $\\theta=\\widehat{\\theta}$, we have\n$$\n\\mathrm{P}\\left(\\left|\\widehat{\\theta}-\\theta_{\\star}\\right| \\geq \\varepsilon\\right) \\leq \\mathrm{P}\\left\\{M(\\widehat{\\theta})<M\\left(\\theta_{\\star}\\right)-\\delta\\right\\} \\rightarrow 0\n$$\n\n## Theorem\n\nThere are many different versions of conditions for proving consistency of MLE. Many of them *rely heavily on the uniform LLN*. We provide an alternative set of conditions.\n\nLet $\\left\\{f_{\\theta}: \\theta \\in \\Theta\\right\\}$ be the model, and $X_{1}, X_{2}, \\ldots \\stackrel{\\text { IID }}{\\sim} f_{\\theta_{\\star}}$. Assume:\n\n1. **Compactness.** $\\Theta$ is compact;\n2. **Uniqueness of maximizer.** $\\theta^{\\star}$ is the unique maximizer of $\\theta \\mapsto M(\\theta)$;\n3. **Continuity.** $M(\\theta)$ is continuous in $\\theta$;\n4. **Uniform LLN.** $\\widehat{M}(\\theta)$ converges uniformly in probability.\n\nThen $\\widehat{\\theta} \\xrightarrow{\\mathrm{pr}} \\theta_{\\star}$.\n\n### Proof\n\nFor $\\epsilon > 0$, define the $\\epsilon$-neighborhood around $\\theta_\\star$ as:\n$$\n\\Theta(\\epsilon)=\\left\\{\\theta:\\left\\|\\theta-\\theta_\\star\\right\\|<\\epsilon\\right\\}\n$$\n\nWe aim to show that:\n$$\nP_{\\theta_0}\\left[\\hat{\\theta} \\in \\Theta(\\epsilon)\\right] \\rightarrow 1\n$$\nSince $\\Theta(\\epsilon)$ is an open set, $\\Theta \\cap \\Theta(\\epsilon)^C$ is compact. Since $M(\\theta)$ is continuous, $\\sup _{\\theta \\in \\Theta \\cap \\Theta(\\epsilon)^C}\\left\\{M(\\theta)\\right\\}$ is achieved for some $\\theta$ in this compact set. Denote this value by $\\theta_0$. Since $\\theta_\\star$ is the unique max, let $M(\\theta_\\star)-M(\\theta_0)=\\delta>0$. For any $\\theta$, we distinguish between two cases:\n\n- $\\theta \\in \\Theta \\cap \\Theta(\\epsilon)^C$.\n\nLet $A_n$ be the event that $\\sup _{\\theta \\in \\Theta \\cap \\Theta(\\epsilon)^C}\\left|M(\\theta)-\\widehat{M}(\\theta)\\right|<\\delta / 2$. Then\n$$\nA_n \\Rightarrow \\widehat{M}\\left(\\theta\\right) <M(\\theta)+\\delta / 2 \\leq M\\left(\\theta_0\\right)+\\delta / 2 =M\\left(\\theta_\\star\\right)-\\delta+\\delta / 2 =M\\left(\\theta_\\star\\right)-\\delta / 2\n$$\n\n- $\\theta \\in \\Theta(\\epsilon)$.\n\nLet $B_n$ be the event that $\\sup _{\\Theta(\\epsilon)}\\left|M(\\theta)-\\widehat{M}(\\theta)\\right|<\\delta / 2$. Then\n$$\nB_n \\Rightarrow \\widehat{M}\\left(\\theta \\right) > M(\\theta) - \\delta / 2 \\quad \\text{for all } \\theta \\\\\n\\Rightarrow \\widehat{M}\\left(\\theta \\right) > M\\left(\\theta_\\star\\right) - \\delta / 2\n$$\n\nWe conclude that if both $A_n$ and $B_n$ hold, then $\\hat{\\theta}\\in \\Theta(\\epsilon)$. By the proof of theorem \\ref{consistency one}, we know that as long as $\\widehat{M}(\\theta)$ converges uniformly in probability, $M\\left(\\theta_{\\star}\\right)-M(\\widehat{\\theta})\\xrightarrow{\\mathrm{pr}} M(\\theta)$. Comparing the two cases above, we have $\\hat{\\theta} \\in \\Theta(\\epsilon)$.\n\n\n\n## Lemma\n\nA key element of the above two proofs is the uniform convergence in probability. However, proving this can be challenging.\n\nLet $\\left\\{f_{\\theta}: \\theta \\in \\Theta\\right\\}$ be the model, and $X_{1}, X_{2}, \\ldots \\stackrel{\\text { IID }}{\\sim} f_{\\theta_{\\star}}$. Assume:\n\n- $\\Theta$ is compact,\n- $\\log f(x ; \\theta)$ is continuous in $\\theta$ for all $\\theta \\in \\Theta$ and all $x \\in \\mathcal{X}$,\n- There exists a function $d(x)$ such that $|\\log f(x ; \\theta)| \\leq d(x)$ for all $\\theta \\in \\Theta$ and $x \\in \\mathcal{X}$, and $E_{\\theta_0}[d(X)]<\\infty$.\n\nThen:\n\n- $M(\\theta)$ is continuous in $\\theta$;\n- $\\sup _{\\theta \\in \\Theta}\\left|\\widehat{M}(\\theta)-M(\\theta)\\right| \\xrightarrow{\\mathrm{pr}} 0$.\n\n### Proof\n\nTo establish continuity, we need to show that if $\\theta_k \\to \\theta$, then $M(\\theta_k) \\to M(\\theta)$. Specifically, we need to demonstrate that\n$$\nM(\\theta_k)= \\operatorname{E}\\left(\\log \\left(\\frac{f_{\\theta_k}}{f_{\\theta_{\\star}}}\\right)\\right) \\rightarrow M(\\theta)= \\operatorname{E}\\left(\\log \\left(\\frac{f_\\theta}{f_{\\theta_{\\star}}}\\right)\\right)\n$$\nBy the continuity of $f_\\theta$, we know that $\\log f_{\\theta_k} \\to \\log f_{\\theta}$. Furthermore, since $\\log f_\\theta \\leq d(X)$ and $ \\operatorname{E} d(X) < \\infty$, we can apply the dominated convergence theorem, which ensures the desired convergence holds. Since $\\Theta$ is compact, $M(\\theta)$ is uniformly continuous.\n\nRegarding uniform convergence, we need to show that:\n$$\n\\sup _{\\theta \\in \\Theta}\\left|\\widehat{M}(\\theta)-M(\\theta)\\right| \\xrightarrow{\\mathrm{pr}} 0\n$$\nWe have shown that $M(\\theta)$ is uniformly continuous. Now, consider the properties of $\\widehat{M}(\\theta)$. Since $\\widehat{M}(\\theta)$ is the average of the log-likelihood, its behavior is closely related to the properties of the likelihood function $f_\\theta(x)$. As $f_\\theta(x)$ is continuous in $\\Theta$ and $\\Theta$ is compact, $\\log f_\\theta(x)$ is uniformly continuous. Thus, for any $\\epsilon > 0$, there exists $\\delta(\\epsilon)$ such that if $\\|\\theta_1 - \\theta_2\\| < \\delta$, then\n$$\n\\sup_{\\|\\theta_1-\\theta_2\\|<\\delta}\\left|\\log f_{\\theta_1}(x)-\\log f_{\\theta_2}(x)\\right|<\\epsilon\n$$\nwhich implies that:\n$$\n\\Delta(x,\\delta)=\\sup_{\\|\\theta_1-\\theta_2\\|<\\delta}\\left|\\log f_{\\theta_1}(x)-\\log f_{\\theta_2}(x)\\right|\\to 0\n$$\n\nTherefore, if $\\|\\theta_1 - \\theta_2\\| < \\delta$ for all $x \\in \\mathcal{X}$, we obtain:\n$$\n\\begin{aligned}\n\\left|\\widehat{M}(\\theta_1)-\\widehat{M}(\\theta_2)\\right| &=\\left| \\frac{1}{n}\\sum_{i=1}^n \\left(\\log f_{\\theta_1}(x_i) -\\log f_{\\theta_2}(x_i)\\right)\\right| \\\\\n&\\leq \\frac{1}{n}\\sum_{i=1}^n \\left|\\log f_{\\theta_1}(x) -\\log f_{\\theta_2}(x)\\right| \\\\\n&\\leq \\frac{1}{n}\\sum_{i=1}^n \\Delta(x,\\delta) \\to 0\n\\end{aligned}\n$$\nSo $\\widehat{M}(\\theta)$ is also uniformly continuous.\n\nNow we can magnify the target inequality and use the properties of uniform continuity.\n\nTo begin, we first “cut off” our set $\\Theta$ by considering an open ball of radius $\\delta$ around each $\\theta \\in \\Theta$,i.e., $B(\\theta, \\delta) = \\{\\tilde{\\theta} : \\|\\tilde{\\theta} - \\theta\\| < \\delta\\}$. The union of these balls forms an open cover of $\\Theta$. By compactness, we can find a finite subcover, denoted as $\\{B(\\theta_j, \\delta), j = 1, \\dots, J\\}$. For each $\\theta \\in \\Theta$, there exists a $\\theta_j$ such that $\\theta \\in B(\\theta_j, \\delta)$. Therefore, for any $\\theta \\in \\Theta$, we have:\n$$\n\\left|\\widehat{M}(\\theta)-M(\\theta)\\right| \\leq \\left|\\widehat{M}(\\theta)-\\widehat{M}(\\theta_j)\\right| + \\left|\\widehat{M}(\\theta_j)-M(\\theta_j)\\right| + \\left|M(\\theta_j)-M(\\theta)\\right|\n$$\nSince $\\widehat{M}(\\theta)$ and $M(\\theta)$ are uniformly continuous, we can choose $\\delta$ small enough such that both $\\left|\\widehat{M}(\\theta)-\\widehat{M}(\\theta_j)\\right|$ and $\\left|M(\\theta_j)-M(\\theta)\\right|$ can be bounded by $\\epsilon/3$.\n\nThus:\n$$\n\\left|\\widehat{M}(\\theta)-M(\\theta)\\right| \\leq \\epsilon/3 + \\max_{j=1,2,\\cdots,J}\\left|\\widehat{M}(\\theta_j)-M(\\theta_j)\\right| + \\epsilon/3\n$$\nwhich implies that:\n$$\n\\sup_{\\theta\\in \\Theta}\\left|\\widehat{M}(\\theta)-M(\\theta)\\right| \\leq 2\\epsilon/3 + \\max_{j=1,2,\\cdots,J}\\left|\\widehat{M}(\\theta_j)-M(\\theta_j)\\right|\n$$\nFurther, we have:\n$$\n\\sup_{\\theta\\in \\Theta}\\left|\\widehat{M}(\\theta)-M(\\theta)\\right| > \\epsilon \\implies \\max_{j=1,2,\\cdots,J}\\left|\\widehat{M}(\\theta_j)-M(\\theta_j)\\right| > \\epsilon/3\n$$\n\nWe now show that:\n$$\n \\operatorname{P} \\left(\\max_{j=1,2,\\cdots,J}\\left|\\widehat{M}(\\theta_j)-M(\\theta_j)\\right| > \\epsilon/3\\right) \\xrightarrow{\\mathrm{pr}} 0\n$$\n\nIt follows that:\n$$\n\\begin{aligned}\n \\operatorname{P} \\left(\\max_{j=1,2,\\cdots,J}\\left|\\widehat{M}(\\theta_j)-M(\\theta_j)\\right| > \\epsilon/3\\right) &=  \\operatorname{P}\\left(\\bigcup_{j=1,2,\\cdots,J}\\left\\{\\left|\\widehat{M}(\\theta_j)-M(\\theta_j)\\right| > \\epsilon/3 \\right\\}\\right) \\\\\n&\\leq \\sum_{j=1,2,\\cdots,J} \\operatorname{P} \\left( \\left\\{\\left|\\widehat{M}(\\theta_j)-M(\\theta_j)\\right| > \\epsilon/3 \\right\\}\\right)\n\\end{aligned}\n$$\n\nBy the WLLN, we know that for each $\\theta_j$ and for any $\\epsilon>0,\\eta>0$, there exists $N(\\epsilon,\\eta)$ such that for all $n>N_j(\\epsilon,\\eta)$:\n$$\n\\operatorname{P} \\left( \\left\\{\\left|\\widehat{M}(\\theta_j)-M(\\theta_j)\\right| > \\epsilon/3 \\right\\}\\right) < \\eta/J\n$$\nLet $N=\\max N_j$, so we have:\n$$\n\\sum_{j=1,2,\\cdots,J} \\operatorname{P} \\left( \\left\\{\\left|\\widehat{M}(\\theta_j)-M(\\theta_j)\\right| > \\epsilon/3 \\right\\}\\right) < \\eta\n$$\n\nFinally:\n$$\n \\operatorname{P}\\left(\\sup _{\\theta \\in \\Theta}\\left|\\widehat{M}(\\theta)-M(\\theta)\\right| > \\epsilon\\right) \\leq  \\operatorname{P}\\left(\\max_{j=1,2,\\cdots,J}\\left|\\widehat{M}(\\theta_j)-M(\\theta_j)\\right| > \\epsilon/3 \\right) \\xrightarrow{\\mathrm{pr}} 0\n$$\nwhich completes the proof.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}