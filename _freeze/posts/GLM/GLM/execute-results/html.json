{
  "hash": "638291c4ab7fa1cb2773748ddde9330c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"GLM\"\nauthor: \"Creo Hsia\"\ndate: \"2024-11-14\"\ndescription: \"GLM study record\"\ncategories: [Statistics Estimation, Optimization, MLE]\nformat: \n  html:\n    highlight-style: ayu\n    code-fold: true\n    code-copy: true \n    code-overflow: 'scroll'\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    editor: visual\nknitr:\n  opts_chunk: \n    fig.path: \"image/\"\n    collapse: true\n    comment: \"#>|\"\n---\n\n\n\n## The exponential family\n\nWe say that a class of distributions is in the exponential family if it can be written in the form:\n$$\np(y ; \\eta)=b(y) \\exp \\left(\\eta^T T(y)-a(\\eta)\\right)\n$$\n\n+ $\\eta$ : the natural parameter (also called the canonical parameter)\n+ $T(y)$: sufficient statistic (for the distributions we consider)\n+ $a(\\eta)$: log partition function. The quantity $e^{-a(\\eta)}$ essentially plays the role of a normalization constant\n\nBernoulli distribution is a members of exponential family, since we can write the Bernoulli distribution as: \n$$\n\\begin{aligned}\np(y ; \\phi) & =\\phi^y(1-\\phi)^{1-y} \\\\\n& =\\exp (y \\log \\phi+(1-y) \\log (1-\\phi)) \\\\\n& =\\exp \\left(\\left(\\log \\left(\\frac{\\phi}{1-\\phi}\\right)\\right) y+\\log (1-\\phi)\\right)\n\\end{aligned}\n$$\nThen the natural parameter is given by $\\eta=\\log (\\phi /(1-\\phi))$. **The inverse is the familiar sigmoid function**!\n\n## Constructing GLMs\n\nConsider a classification or regression problem where we would like to predict the value of some random variable $y$ as a function of $x$. To derive a GLM for this problem, we will make the following three assumptions about the conditional distribution of $y$ given $x$ and about our model:\n\n1. $y \\mid x ; \\theta \\sim \\operatorname{ExponentialFamily}(\\eta)$. I.e., given $x$ and $\\theta$, the distribution of $y$ follows some exponential family distribution, with parameter $\\eta$.\n2. Given $x$, our goal is to predict the expected value of $T(y)$ given $x$. \n3. The natural parameter $\\eta$ and the inputs $x$ are related linearly: $\\eta=\\theta^T x$. (Or, if $\\eta$ is vector-valued, then $\\eta_i=\\theta_i^T x$.)\n\nNow we show that logistic regression is a member of GLMs. Note that if $y|x;\\theta \\sim \\text{Bernouli}(\\phi)$, then $Ey|x;\\theta=\\phi$. So\n$$\n\\begin{aligned}\nh_\\theta(x) & =E[y \\mid x ; \\theta]  \\\\\n& =\\phi (\\text{ by Bernoulli assumption})\\\\\n& =1 /\\left(1+e^{-\\eta}\\right) (\\text{ by exponential familay})\\\\\n& =1 /\\left(1+e^{-\\theta^T x}\\right) (\\text{ by assumption 3})\n\\end{aligned}\n$$\n\n> Once we assume that $y$ conditioned on $x$ is Bernoulli, logistic regression arises as a consequence of the definition of GLMs and exponential family distributions.\n\nOften, $\\mathrm{E}[T(y) ; \\eta]$ is a function of $\\eta$ (denoted by $g(\\eta)$). We call it the **canoniucal response function**. Its inverse, $g^{-1}$, is called the canonical link function.\n\nConsider a classification problem in which the response variable $y$ can take on any one of $k$ values. The response variable is still **discrete**, but can now take on **more than two values**. We will thus model it as distributed according to a **multinomial distribution**.\n\nTo do so, we will begin by **expressing the multinomial as an exponential family distribution**.\n\nTo parameterize a multinomial over $k$ possible outcomes, we can parameterize the multinomial with only $k-1$ parameters, $\\phi_1, \\ldots, \\phi_{k-1}$, where $\\phi_i=p(y=i ; \\phi)$, and $p(y=k ; \\phi)=1-\\sum_{i=1}^{k-1} \\phi_i$. \n\nNow define \n$$\nT(1)=\\left[\\begin{array}{c}\n1 \\\\\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right], T(2)=\\left[\\begin{array}{c}\n0 \\\\\n1 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right], T(3)=\\left[\\begin{array}{c}\n0 \\\\\n0 \\\\\n1 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right], \\cdots, T(k-1)=\\left[\\begin{array}{c}\n0 \\\\\n0 \\\\\n0 \\\\\n\\vdots \\\\\n1\n\\end{array}\\right], T(k)=\\left[\\begin{array}{c}\n0 \\\\\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right],\n$$\nWe are now ready to show that the multinomial is a member of the exponential family. We have \n$$\n\\begin{aligned}\np(y ; \\phi)= & \\phi_1^{1\\{y=1\\}} \\phi_2^{1\\{y=2\\}} \\cdots \\phi_k^{1\\{y=k\\}} \\\\\n= & \\phi_1^{1\\{y=1\\}} \\phi_2^{1\\{y=2\\}} \\cdots \\phi_k^{1-\\sum_{i=1}^{k-1} 1\\{y=i\\}} \\\\\n= & \\phi_1^{(T(y))_1} \\phi_2^{(T(y))_2} \\cdots \\phi_k^{1-\\sum_{i=1}^{k-1}(T(y))_i} \\\\\n= & \\exp \\left((T(y))_1 \\log \\left(\\phi_1\\right)+(T(y))_2 \\log \\left(\\phi_2\\right)+\\right. \\\\\n& \\left.\\quad \\cdots+\\left(1-\\sum_{i=1}^{k-1}(T(y))_i\\right) \\log \\left(\\phi_k\\right)\\right) \\\\\n= & \\exp \\left((T(y))_1 \\log \\left(\\phi_1 / \\phi_k\\right)+(T(y))_2 \\log \\left(\\phi_2 / \\phi_k\\right)+\\right. \\\\\n& \\left.\\quad \\cdots+(T(y))_{k-1} \\log \\left(\\phi_{k-1} / \\phi_k\\right)+\\log \\left(\\phi_k\\right)\\right) \\\\\n= & b(y) \\exp \\left(\\eta^T T(y)-a(\\eta)\\right)\n\\end{aligned}\n$$\nWhere,\n$$\n\\begin{aligned}\n\\eta & =\\left[\\begin{array}{c}\n\\log \\left(\\phi_1 / \\phi_k\\right) \\\\\n\\log \\left(\\phi_2 / \\phi_k\\right) \\\\\n\\vdots \\\\\n\\log \\left(\\phi_{k-1} / \\phi_k\\right)\n\\end{array}\\right] \\\\\na(\\eta) & =-\\log \\left(\\phi_k\\right) \\\\\nb(y) & =1\n\\end{aligned}\n$$\nNote that $E T_i = \\phi_i=\\phi_k e^{\\eta_i}$, hence the link function is given by \n$$\n\\eta_i=\\log \\frac{\\phi_i}{\\phi_k}\n$$\nDefine $\\eta_k=0$, we have that\n$$\n1=\\sum_{i=1}^k\\phi_i=\\sum_{i=1}^k\\phi_ke^{\\eta_i}\n$$\nHence $\\phi_k=1 / \\sum_{i=1}^k e^{\\eta_i}$, further\n$$\n\\phi_i=\\frac{e^{\\eta_i}}{\\sum_{j=1}^k e^{\\eta_j}}\n$$\nBy assumption 3, $\\eta_i$'s are linearly related to the $x$'s. Hence, our model assumes that the conditional distribution of $y$ given $x$ is given by\n$$\n\\begin{aligned}\np(y=i \\mid x ; \\theta) & =\\phi_i \\\\\n& =\\frac{e^{\\eta_i}}{\\sum_{j=1}^k e^{\\eta_j}} \\\\\n& =\\frac{e^{\\theta_i^T x}}{\\sum_{j=1}^k e^{\\theta_j^T x}}\n\\end{aligned}\n$$\n\n> Here we define $\\theta_k=0$. Our model will output the estimated probability that $p(y=i \\mid x ; \\theta)$\n\n\n\nNow we consider the parameter fitting. We would begin by writing down the log-likelihood\n$$\n\\begin{aligned}\n\\ell(\\theta) & =\\sum_{i=1}^n \\log p\\left(y^{(i)} \\mid x^{(i)} ; \\theta\\right) \\\\\n& =\\sum_{i=1}^n \\log \\prod_{l=1}^k\\left(\\frac{e^{\\theta_l^T x^{(i)}}}{\\sum_{j=1}^k e^{\\theta_j^T x^{(i)}}}\\right)^{1\\left\\{y^{(i)}=l\\right\\}}\n\\end{aligned}\n$$\nWe can now obtain the maximum likelihood estimate of the parameters by maximizing $\\ell(\\theta)$ in terms of Î¸, using a method such as gradient ascent or Newton's method.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}