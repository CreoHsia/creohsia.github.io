{
  "hash": "30fafa6b10a44c14aa770777ebf704a3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Optimization Final Project\"\nauthor: \"Creo Hsia\"\ndate: \"2024-11-14\"\ndescription: \"Draft about optimization theory fianl project\"\ncategories: [Statistics Estimation, Optimization, MLE]\nformat: \n  html:\n    highlight-style: ayu\n    code-fold: true\n    code-copy: true \n    code-overflow: 'scroll'\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    editor: visual\nknitr:\n  opts_chunk: \n    fig.path: \"image/\"\n    collapse: true\n    comment: \"#>|\"\n---\n\n\n\n## Model\nAssume that $N_i|b_{i1},b_{i2};\\alpha_1,\\alpha_2\\sim \\textnormal{Poisson}(\\lambda_i)$, where $\\lambda_i=\\alpha_1 b_{i1}+\\alpha_2 b_{i2}$. Then, the PMF (Probability Mass Function) is \n$$\nP(N_i=n_i)=\\frac{(\\alpha_1 b_{i1}+\\alpha_2 b_{i2})^{n_i}}{n_i!}\\exp\\{-(\\alpha_1 b_{i1}+\\alpha_2 b_{i2}) \\}\n$$ \nand the log-likelihood is \n$$\n\\ell(\\alpha_1,\\alpha_2|N_i, b_{i1},b_{i2})=N_i\\log (\\alpha_1 b_{i1}+\\alpha_2 b_{i2})-(\\alpha_1 b_{i1}+\\alpha_2 b_{i2})-\\log(N_1!)\n$$\n\nHence the log-likelihood of the data is \n$$\n\\ell(\\alpha_1,\\alpha_2|N_{1:m}, b_{11:m1},b_{12:m2})=\\sum_{i=1}^mN_i\\log (\\alpha_1 b_{i1}+\\alpha_2 b_{i2})-(\\alpha_1 \\sum_{i=1}^mb_{i1}+\\alpha_2 \\sum_{i=1}^mb_{i2})-\\sum_{i=1}^m\\log(N_i!)\n$$ \n\n> Here $b_{11:m1}$ means $b_{11},b_{21}\\ldots b_{m1}$.\n\nTo make our notation tidiness, define \n$$\nB_{1m}=\\sum_{i=1}^mb_{i1},\\quad B_{2m}=\\sum_{i=1}^mb_{i2},\\quad C_m=\\sum_{i=1}^m\\log(N_i!)\n$$ \nHence \n$$\n\\ell(\\alpha_1,\\alpha_2|N_{1:m}, b_{11:1m},b_{12:2m})=\\sum_{i=1}^mN_i\\log (\\alpha_1 b_{i1}+\\alpha_2 b_{i2})-\\alpha_1B_{1m}-\\alpha_2B_{2m}-C_m\n$$ \nThe first derivation is \n$$\n\\nabla_{\\boldsymbol{\\alpha}} \\ell =\\begin{pmatrix}\\frac{\\partial \\ell}{\\partial \\alpha_1}\\\\\n\\frac{\\partial \\ell}{\\partial \\alpha_1}\\end{pmatrix}= \\begin{pmatrix}\\sum_{i=1}^m\\frac{N_ib_{i1}}{\\alpha_1 b_{i1}+\\alpha_2 b_{i2}}-B_{1m}\\\\\n\\sum_{i=1}^m\\frac{N_ib_{i2}}{\\alpha_1 b_{i1}+\\alpha_2 b_{i2}}-B_{2m}\\end{pmatrix}\n$$ \nAnd the Hessian is \n$$\n\\begin{aligned}\n\\nabla_{\\boldsymbol{\\alpha}}\\nabla^T_{\\boldsymbol{\\alpha}} \\ell=&\\begin{pmatrix} \\frac{\\partial\\ell^2}{\\partial^2 \\alpha_1}& \\frac{\\partial\\ell^2}{\\partial \\alpha_1\\partial \\alpha_2}\\\\\n\\frac{\\partial\\ell^2}{\\partial \\alpha_2\\partial \\alpha_1}&\\frac{\\partial\\ell^2}{\\partial^2 \\alpha_2}\\end{pmatrix}\\\\\n=&\\begin{pmatrix} -\\sum_{i=1}^m\\frac{N_{i1}b_{i1}b_{i1}}{(\\alpha_1 b_{i1}+\\alpha_2 b_{i2})^2} &-\\sum_{i=1}^m\\frac{N_{i1}b_{i1}b_{i2}}{(\\alpha_1 b_{i1}+\\alpha_2 b_{i2})^2}\\\\\n-\\sum_{i=1}^m\\frac{N_{i1}b_{i2}b_{i1}}{(\\alpha_1 b_{i1}+\\alpha_2 b_{i2})^2}& -\\sum_{i=1}^m\\frac{N_{i1}b_{i2}b_{i2}}{(\\alpha_1 b_{i1}+\\alpha_2 b_{i2})^2}\n\\end{pmatrix}\n\\end{aligned}\n$$ \n\nHence the Fisher score is \n\n$$\n\\begin{aligned}\nI(\\alpha)=&-E\\nabla_{\\boldsymbol{\\alpha}}\\nabla^T_{\\boldsymbol{\\alpha}} \\ell\\\\\n=&\\begin{pmatrix}\n\\sum_{i=1}^m\\frac{b_{i1}b_{i1}}{\\alpha_1 b_{i1}+\\alpha_2 b_{i2}} &\\sum_{i=1}^m\\frac{b_{i1}b_{i2}}{\\alpha_1 b_{i1}+\\alpha_2 b_{i2}} \\\\\n\\sum_{i=1}^m\\frac{b_{i2}b_{i1}}{\\alpha_1 b_{i1}+\\alpha_2 b_{i2}} & \\sum_{i=1}^m\\frac{b_{i2}b_{i2}}{\\alpha_1 b_{i1}+\\alpha_2 b_{i2}}\n\\end{pmatrix}\n\\end{aligned}\n$$\n\n## Algorithm\n\nLoad data (click to down load [data](oilspills.dat) and [description](oilspills.txt))\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noilspills <- read.csv(\"oilspills.dat\", sep=\"\")\nN <- oilspills$spills\nb1 <- oilspills$importexport\nb2 <- oilspills$domestic\n\n# Initial guess for alpha values\ninitial_alpha <- c(1,1 )\n```\n:::\n\n\n\nDefine some function\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to calculate the log-likelihood\nlog_likelihood <- function(alpha, N, b1, b2) {\n  lambda <- alpha[1] * b1 + alpha[2] * b2\n  sum(dpois(N, lambda,log = TRUE))\n}\n\n# Function to calculate the gradient\ngradient <- function(alpha, N, b1, b2) {\n  lambda <- alpha[1] * b1 + alpha[2] * b2\n  grad1 <- sum((N * b1) / lambda - b1)\n  grad2 <- sum((N * b2) / lambda - b2)\n  return(c(grad1, grad2))\n}\n\n# Function to calculate the Hessian matrix\nhessian <- function(alpha, N, b1, b2) {\n  lambda <- alpha[1] * b1 + alpha[2] * b2\n  h11 <- -sum((N * b1^2) / (lambda^2))\n  h12 <- -sum((N * b1 * b2) / (lambda^2))\n  h21 <- h12\n  h22 <- -sum((N * b2^2) / (lambda^2))\n  return(matrix(c(h11, h12, h21, h22), nrow = 2))\n}\n\n# Function to calculate the Fisher information\nfisher_information <- function(alpha, b1, b2) {\n  lambda <- alpha[1] * b1 + alpha[2] * b2\n  fisher_info <- matrix(0, nrow = 2, ncol = 2)\n  fisher_info[1, 1] <- sum(b1^2 / lambda)\n  fisher_info[1, 2] <- sum(b1 * b2 / lambda)\n  fisher_info[2, 1] <- fisher_info[1, 2]\n  fisher_info[2, 2] <- sum(b2^2 / lambda)\n  return(fisher_info)\n}\n\n```\n:::\n\n\n\n\nNewton method\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Update the Newton-Raphson method to include iteration number in the path matrix\nnewton_raphson <- function(N, b1, b2, initial_alpha, tol = 1e-6, max_iter = 100) {\n  alpha <- initial_alpha  # Initial parameter values\n  path_matrix <- matrix(NA, nrow = max_iter + 1, ncol = 3)  # Matrix to store iteration number and alpha values\n  path_matrix[1, ] <- c(1, alpha)  # Store initial alpha values at iteration 1\n  \n  for (i in 1:max_iter) {\n    # Compute gradient and Hessian at the current alpha\n    grad <- gradient(alpha, N, b1, b2)\n    hess <- hessian(alpha, N, b1, b2)\n    \n    # Calculate the step size using the inverse of the Hessian and gradient\n    step <- solve(hess) %*% grad\n    \n    # Update alpha by subtracting the step size (moving towards the maximum)\n    alpha_new <- alpha - step\n    \n    # Store the new alpha values and the iteration number in the path matrix\n    path_matrix[i + 1, ] <- c(i + 1, alpha_new)\n    \n    # Check for convergence based on the relative change in alpha\n    relative_change <- sqrt(sum((alpha_new - alpha)^2)) / sqrt(sum(alpha^2))\n    if (relative_change < tol) break  # Stop if change is below the tolerance threshold\n    \n    alpha <- alpha_new  # Update alpha for the next iteration\n  }\n  \n  # Trim the path matrix to include only completed iterations\n  path_matrix <- path_matrix[1:(i + 1), , drop = FALSE]\n  return(list(estimate = alpha, path = path_matrix,method=\"Newton\"))  # Return final alpha and path\n}\n```\n:::\n\n\n\n\nFisher score method\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Update the Fisher scoring method to include iteration number in the path matrix\nfisher_scoring <- function(N, b1, b2, initial_alpha, tol = 1e-6, max_iter = 100) {\n  alpha <- initial_alpha  # Initial parameter values\n  path_matrix <- matrix(NA, nrow = max_iter + 1, ncol = 3)  # Matrix to store iteration number and alpha values\n  path_matrix[1, ] <- c(1, alpha)  # Store initial alpha values at iteration 1\n  \n  for (i in 1:max_iter) {\n    # Compute gradient and Fisher information at the current alpha\n    grad <- gradient(alpha, N, b1, b2)\n    fisher_info <- fisher_information(alpha, b1, b2)\n    \n    # Calculate the step size using the inverse of the Fisher information and gradient\n    step <- solve(fisher_info) %*% grad\n    \n    # Update alpha by adding the step size (moving towards the maximum)\n    alpha_new <- alpha + step\n    \n    # Store the new alpha values and the iteration number in the path matrix\n    path_matrix[i + 1, ] <- c(i + 1, alpha_new)\n    \n    # Check for convergence based on the relative change in alpha\n    relative_change <- sqrt(sum((alpha_new - alpha)^2)) / sqrt(sum(alpha^2))\n    if (relative_change < tol) break  # Stop if change is below the tolerance threshold\n    \n    alpha <- alpha_new  # Update alpha for the next iteration\n  }\n  \n  # Trim the path matrix to include only completed iterations\n  path_matrix <- path_matrix[1:(i + 1), , drop = FALSE]\n  return(list(estimate = alpha, path = path_matrix,method=\"Fisher score\"))  # Return final alpha and path\n}\n```\n:::\n\n\n\n\nSteepest ascent method\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Update the steepest ascent method to include iteration number in the path matrix\nsteepest_ascent <- function(N, b1, b2, initial_alpha, tol = 1e-6, max_iter = 300, step_size = 0.01) {\n  alpha <- initial_alpha  # Initial parameter values\n  path_matrix <- matrix(NA, nrow = max_iter + 1, ncol = 3)  # Matrix to store iteration number and alpha values\n  path_matrix[1, ] <- c(1, alpha)  # Store initial alpha values at iteration 1\n  \n  for (i in 1:max_iter) {\n    # Compute gradient at the current alpha\n    grad <- gradient(alpha, N, b1, b2)\n    \n    # Update alpha in the direction of the gradient, scaled by step size\n    alpha_new <- alpha + step_size * grad\n    \n    # Store the new alpha values and the iteration number in the path matrix\n    path_matrix[i + 1, ] <- c(i + 1, alpha_new)\n    \n    # Check for convergence based on the relative change in alpha\n    relative_change <- sqrt(sum((alpha_new - alpha)^2)) / sqrt(sum(alpha^2))\n    if (relative_change < tol) break  # Stop if change is below the tolerance threshold\n    \n    alpha <- alpha_new  # Update alpha for the next iteration\n  }\n  \n  # Trim the path matrix to include only completed iterations\n  path_matrix <- path_matrix[1:(i + 1), , drop = FALSE]\n  return(list(estimate = alpha, path = path_matrix,method=\"steepest_ascent\"))  # Return final alpha and path\n}\n```\n:::\n\n\n\n\nsteepest ascent with step-halving\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Update the steepest ascent with step-halving method to include iteration number in the path matrix\nsteepest_ascent_halve <- function(N, b1, b2, initial_alpha, tol = 1e-6, max_iter = 100) {\n  alpha <- initial_alpha  # Initial parameter values\n  path_matrix <- matrix(NA, nrow = max_iter + 1, ncol = 3)  # Matrix to store iteration number and alpha values\n  path_matrix[1, ] <- c(1, alpha)  # Store initial alpha values at iteration 1\n  \n  for (i in 1:max_iter) {\n    # Compute gradient at the current alpha\n    grad <- gradient(alpha, N, b1, b2)\n    \n    # Start with an initial step size of 1\n    step_size <- 1\n    alpha_new <- alpha + step_size * grad\n    \n    # Apply step-halving if the new alpha does not improve the log-likelihood\n    while (log_likelihood(alpha_new, N, b1, b2) < log_likelihood(alpha, N, b1, b2)) {\n      step_size <- step_size / 2  # Halve the step size\n      alpha_new <- alpha + step_size * grad  # Update alpha with the reduced step size\n    }\n    \n    # Store the new alpha values and the iteration number in the path matrix\n    path_matrix[i + 1, ] <- c(i + 1, alpha_new)\n    \n    # Check for convergence based on the relative change in alpha\n    relative_change <- sqrt(sum((alpha_new - alpha)^2)) / sqrt(sum(alpha^2))\n    if (relative_change < tol) break  # Stop if change is below the tolerance threshold\n    \n    alpha <- alpha_new  # Update alpha for the next iteration\n  }\n  \n  # Trim the path matrix to include only completed iterations\n  path_matrix <- path_matrix[1:(i + 1), , drop = FALSE]\n  return(list(estimate = alpha, path = path_matrix,method=\"steepest_ascent_with_step-halving\"))  # Return final alpha and path\n}\n```\n:::\n\n\n\n\n\nRun all\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Run all methods\nresult_newton <- newton_raphson(N, b1, b2, initial_alpha)\nresult_fisher <- fisher_scoring(N, b1, b2, initial_alpha)\nresult_steepest_ascent <- steepest_ascent(N, b1, b2, initial_alpha)\nresult_steepest_ascent_halve <- steepest_ascent_halve(N, b1, b2, initial_alpha)\n```\n:::\n\n\n\nPlot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n# Convert path matrices to data frames for plotting\nconvert_path_matrix_to_df <- function(path_matrix, method) {\n  data.frame(\n    iteration = path_matrix[, 1],\n    alpha1 = path_matrix[, 2],\n    alpha2 = path_matrix[, 3],\n    method = method\n  )\n}\n# Convert paths to data frames\npath_newton_df <- convert_path_matrix_to_df(result_newton$path, \"Newton-Raphson\")\npath_fisher_df <- convert_path_matrix_to_df(result_fisher$path, \"Fisher Scoring\")\npath_steepest_ascent_df <- convert_path_matrix_to_df(result_steepest_ascent$path, \"Steepest Ascent\")\npath_steepest_ascent_halve_df <- convert_path_matrix_to_df(result_steepest_ascent_halve$path, \"Steepest Ascent (Step-Halving)\")\n# Combine all paths into a single data frame\npath_df <- rbind(path_newton_df, \n                 path_fisher_df, \n                 path_steepest_ascent_df, \n                 path_steepest_ascent_halve_df)\n\n```\n:::\n\n\n\nPlot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a grid for alpha1 and alpha2 values for the contour plot\nalpha1_seq <- seq(0.99, 1.12, length.out = 100)\nalpha2_seq <- seq(0.88, 1.05, length.out = 100)\ngrid <- expand.grid(alpha1 = alpha1_seq, alpha2 = alpha2_seq)\n# Function to calculate the log-likelihood for the grid using dpois\nlikelihood_grid <- function(alpha1, alpha2, N, b1, b2) {\n  alpha <- c(alpha1, alpha2)\n  lambda <- alpha[1] * b1 + alpha[2] * b2\n  sum(dpois(N, lambda, log = TRUE))\n}\n# Calculate log-likelihood values for the grid\ngrid$likelihood <- apply(grid, 1, function(row) {\n  likelihood_grid(row[\"alpha1\"], row[\"alpha2\"], N, b1, b2)\n})\n# Plot the contour and paths\nggplot() +\n  geom_contour(data = grid, aes(x = alpha1, y = alpha2, z = likelihood), bins = 150, color = \"blue\") +\n  geom_point(data = path_df, aes(x = alpha1, y = alpha2, color = method), size = 2) +\n  geom_path(data = path_df, aes(x = alpha1, y = alpha2, color = method, group = method), linetype = \"dashed\") +\n  theme_minimal() +\n  labs(\n    title = \"Contour Plot of Log-Likelihood with Paths of Optimization Methods\",\n    x = expression(alpha[1]),\n    y = expression(alpha[2]),\n    color = \"Method\"\n  )\n```\n\n::: {.cell-output-display}\n![](image/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nEstimate sd\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Function to estimate standard errors from the Fisher information matrix\nestimate_standard_errors <- function(alpha_mle, b1, b2) {\n  fisher_info <- fisher_information(alpha_mle, b1, b2)\n  fisher_info_inv <- solve(fisher_info)  # Inverse of the Fisher information matrix\n  standard_errors <- sqrt(diag(fisher_info_inv))  # Square root of the diagonal elements\n  return(standard_errors)\n}\n\n# Use the MLEs from both the Newton-Raphson and Fisher scoring methods\nalpha_mle_newton <- result_newton$estimate\n\nstandard_errors_newton <- estimate_standard_errors(alpha_mle_newton, b1, b2)\ncat(\"Standard errors for the MLEs (alpha_1 and alpha_2) using Newton-Raphson:\\n\",\n    standard_errors_newton,\"\\n\")\n#>| Standard errors for the MLEs (alpha_1 and alpha_2) using Newton-Raphson:\n#>|  0.437556 0.6314687\n```\n:::\n\n\n\n\niteration\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint_iteration_counts <- function(results_list) {\n  for (result in results_list) {\n    cat(\"Iterations for\", result$method,\"method:\", nrow(result$path),\"\\n\")\n  }\n}\n# Create a list of results for iteration summary\nresults_list <- list(result_newton, result_fisher, result_steepest_ascent, result_steepest_ascent_halve)\n\n# Print iteration counts for each method\nprint_iteration_counts(results_list)\n#>| Iterations for Newton method: 5 \n#>| Iterations for Fisher score method: 13 \n#>| Iterations for steepest_ascent method: 301 \n#>| Iterations for steepest_ascent_with_step-halving method: 44\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}