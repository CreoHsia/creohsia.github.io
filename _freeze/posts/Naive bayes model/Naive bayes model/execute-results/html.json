{
  "hash": "292d9e46d6b93f1a83c2c6c53e37f7f5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Naive Bayes Model\"\nauthor: \"Creo Hsia\"\ndate: \"2024-10-14\"\ndescription: \"A self use function for Naive Bayes classifier\"\nformat: \n  html:\n    #code-fold: true # 折叠代码块\n    code-fold: true\n    code-copy: true  # 代码块显示复制按钮\n    code-overflow: 'scroll'\n    toc: true\n    toc-depth: 2\n    number-sections: true\n    editor: visual\n    \nknitr:\n  opts_chunk: \n    collapse: true\n    comment: \"#>\" \n---\n\n\n---------\n\nWe build a Naive Bayes classifier that can handle any classification problem, assuming continuous variables follow a normal distribution. This classifier is not limited to the specific problem in the exercises.\n\n## Function `naiveBayes`\nThe following function fits a Naive Bayes model to the data, estimating prior probabilities and conditional probabilities for both continuous and categorical features. This function takes four inputs: the `data`, the `response` (a `character string` specifying which column contains the class labels), the `prior` (if provided), and the `Laplace smoothing parameter`. The output is a list containing the conditional probabilities for each variable, the prior probabilities, an indicator of which columns are numeric or categorical, and the levels of the class labels. The returned list is of class `naiveBayes`, which allows for method dispatching specific to this model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnaiveBayes <- function(data, response, prior = NULL, alpha = 0) {\n  # Step 1: Extract the target variable (response)\n  # 'label' contains the target class from the 'response' column of the data\n  label <- data[[response]]\n  level <- unique(label)# Identify unique classes in the target variable\n  # Remove the response column from the data \n  #(since it should not be used as a predictor)\n  data[[response]] <- NULL\n  # Step 2: Calculate prior probabilities if not provided\n  if (is.null(prior)) {\n    prior <- prop.table(table(label))\n  }\n  # Step 3: Identify numeric and categorical columns\n  is_num <- sapply(data, function(x) is.numeric(x))\n  # Step 4: Process numeric and categorical columns separately\n  tbl <- lapply(data,function(col){\n    if (is.numeric(col)){\n      # For numeric columns, calculate the mean and variance\n      #for each class in the target variable\n      means <- tapply(col, label, mean,na.rm = TRUE)\n      sd <- tapply(col, label, sd,na.rm = TRUE)\n      cbind(means,sd)\n    }\n    else {\n      # For categorical columns, calculate smoothed counts \n      # and conditional probabilities\n      # Do not display the count of NA values (this is the default behavior)\n      counts <- table(label, col)\n      # Apply Laplace smoothing to avoid zero probabilities\n      smoothed_counts <- counts + alpha \n      # Calculate conditional probabilities\n      smoothed_probs <- prop.table(smoothed_counts, margin = 1)  \n      return(smoothed_probs)\n    }\n  })\n  # Return the model as a list containing: \n  #1) prior probabilities, 2) tables of statistics, 3) and metadata\n  result <- list(\n    prior = prior, tables = tbl, \n    isnumeric=is_num,\n    level=level)\n  # Assign a class to the result for method dispatch\n  class(result) <- \"naiveBayes\"\n  return(result)\n} \n```\n:::\n\n\n\n## Function `predict.naiveBayes`\nThe predict function takes a trained Naive Bayes model and new data as inputs, and returns predicted class labels or class probabilities depending on the specified type. For missing values (`NA`), the function handles them by assigning neutral probabilities (effectively ignoring them in the likelihood calculation).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict.naiveBayes <- function(model, newdata, type = \"class\") {\n  # Step 1: Convert categorical variables to factors with matching levels\n  for (i in names(model$tables)) {\n    if (!model$isnumeric[i]) {\n      # Ensure categorical variables in newdata \n      # have the same levels as those in the model\n      newdata[[i]] <- factor(newdata[[i]],\n                             levels = colnames(model$tables[[i]]))\n    }\n  }\n  \n  # Step 2: Prepare for prediction (pre-compute constants)\n  logprior <- log(model$prior)   \n  len <- length(model$prior)\n  # Match newdata columns with model attributes\n  match_idx <- match(names(model$tables), names(newdata))  \n  \n  # Step 3: Preallocate log probability matrix for faster computation\n  Logprob <- matrix(0, nrow = len, ncol = nrow(newdata))\n  \n  # Step 4: Compute log-probabilities for both numeric and categorical data\n  for (v in seq_along(match_idx)) {\n    nd <- newdata[, match_idx[v]] # Get the relevant column of newdata\n    if (model$isnumeric[v]) {\n      # For numeric columns, calculate Gaussian likelihoods\n      tbl <- model$tables[[v]]\n      means <- tbl[, 1]\n      sd <- tbl[, 2]\n      # Apply Gaussian log-likelihood using vectorized operations\n      Logprob <- Logprob + sapply(nd, function(x) {\n        # If value is NA, return a vector of zeros for all classes\n        if (is.na(x)) {\n          rep.int(0, len)  \n        } else {\n          dnorm(x, mean = means, sd = sd, log = TRUE)# Compute log likelihood\n        }\n      })\n    } else {\n      # For categorical columns, \n      # retrieve the log probabilities from the model's tables\n      Logprob <- Logprob + sapply(nd,function(x){\n        if (is.na(x)){\n          # If value is NA, return a vector of zeros for all classes\n          rep.int(0,len) \n        } else {\n          # Lookup the log-probability \n          # avoid log(0)\n          log(model$tables[[v]][, x]+1e-8)\n        }\n      })\n    }\n  }\n  \n  # Step 5: Add log-priors to the computed log-likelihoods\n  Logprob <- sweep(Logprob, 1, logprior, \"+\")\n  colnames(Logprob) <- NULL\n  # Step 6: Return predictions based on the specified type ('class' or 'raw')\n  if (type == \"class\") {\n    pred_classes <- model$level[apply(Logprob,2,which.max)]\n    return(pred_classes)\n  } else {\n    # If type == \"raw\", return probabilities for all classes\n    probs <- exp(Logprob)  # Convert log-probabilities back to normal scale\n    # Normalize probabilities so they sum to 1\n    probs <- apply(probs,2,function(x)x/sum(x) ) \n    return(t(probs))\n  }\n}\n\n```\n:::\n\n\n\n## Function `confusion_matrix` and `roc_auc`\n\nTwo functions that plots the ROC curve for a binary classification model and compute the confusion matrix. The function takes three inputs: the true labels, the output from the prediction function (which typically returns the predicted probabilities), and the name of the class to be considered as the positive class (set to 1). You can customize this function for multi-class classification by applying a ‘one-vs-all’ approach, where the ROC curve is generated for each class by treating it as the positive class and all other classes as the negative class. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusion_matrix <- function(true_labels, predicted_probs, class_name,threshold=.5){\n  \n  # Convert true labels to binary\n  true_labels <- ifelse(true_labels==class_name,1,0)\n  # Classify based on the current threshold\n  predicted_class <- ifelse(predicted_probs[,class_name] >= threshold, 1, 0)\n  # Calculate TP, FP, TN, FN from true labels and predicted classes\n  TP <- sum(true_labels == 1 & predicted_class == 1)\n  FP <- sum(true_labels == 0 & predicted_class == 1)\n  TN <- sum(true_labels == 0 & predicted_class == 0)\n  FN <- sum(true_labels == 1 & predicted_class == 0)\n  # Return the confusion matrix as a 2x2 matrix\n  matrix(c(TP, FN, FP, TN), nrow = 2, byrow = TRUE,\n         dimnames = list(\"Actual\" = c(\"Positive\", \"Negative\"),\n                         \"Predicted\" = c(\"Positive\", \"Negative\")))\n}\n\n\n\nroc_auc <- function(true_labels, predicted_probs, class_name) {\n  # Convert true labels to binary\n  binary_labels <- ifelse(true_labels==class_name,1,0)\n  # Number of positive examples (M) and negative examples (N)\n  M <- sum(binary_labels)\n  N <- length(binary_labels)-M\n  # Extract predicted probabilities\n  pos_prob <- predicted_probs[,class_name]\n  # Internal function to compute TPR and FPR\n  compute_tpr_fpr <- function(binary_labels,pos_prob , threshold) {\n    # Classify based on the current threshold\n    predicted_class <- ifelse(pos_prob >= threshold, 1, 0)\n    # Calculate TP, FP, TN, FN from true labels and predicted classes\n    TP <- sum(binary_labels == 1 & predicted_class == 1)\n    FP <- sum(binary_labels == 0 & predicted_class == 1)\n    TN <- sum(binary_labels == 0 & predicted_class == 0)\n    FN <- sum(binary_labels == 1 & predicted_class == 0)\n    # Calculate TPR (True Positive Rate) and FPR (False Positive Rate)\n    TPR <- TP / (TP + FN)  \n    FPR <- FP / (FP + TN)  \n    youden<- TPR-FPR\n    distance <- sqrt((1-TPR)^2+FPR^2)\n    return(c(\"TPR\"=TPR,\"FPR\"=FPR,\"treshold\"=threshold,\n             \"youden\"=youden,\"distance\"=distance))\n  }\n  # Define a sequence of threshold values from 0 to 1, with a step of 0.001\n  thresholds <- seq(0, 1, by = 0.001)\n  # Compute TPR and FPR for each threshold\n  roc_data <- vapply(thresholds,function(thresh){\n    compute_tpr_fpr(binary_labels, pos_prob, thresh)\n  },numeric(5))# Specify the result to be length 3 numeric vector\n  # Calculate AUC \n  auc <- (sum(rank(pos_prob)[binary_labels==1])-M*(M+1)/2)/(M*N)\n  # Convert to a data frame for easier use\n  roc_data <- as.data.frame(t(roc_data))\n  # Find the threshold with the maximum Youden's index \n  # and the minimum distance to the (0,1) point\n  tresh1 <- thresholds[which.max(roc_data$youden)]\n  tresh2 <- thresholds[which.min(roc_data$distance)]\n  # Compute confusion matrices for these thresholds\n  conf_matrix_youden <- confusion_matrix(true_labels,predicted_probs,class_name,tresh1)\n  conf_matrix_distance <- confusion_matrix(true_labels,predicted_probs,class_name,tresh2)\n  p <- ggplot(roc_data, aes(x =FPR, y =TPR)) +\n    geom_line() +  \n    geom_abline(slope = 1,intercept = 0,linetype=\"dashed\",color=\"gray\") + \n    labs(x = \"False Positive Rate\", y = \"True Positive Rate\", \n         title = \"ROC Curves\") + \n    theme_minimal()+\n    # Annotate AUC on the plot\n    annotate(\"text\", x = 0.7, y = 0.1, \n             label = paste(\"AUC =\", round(auc, 4)),\n             size = 5, color = \"blue\")\n  return(list(\n    auc = auc,\n    optimal_tresholds = c(\"youden\"=tresh1,\"distance\"=tresh2),\n    confusion_matrix=list(\n      \"youden\"=conf_matrix_youden,\n      \"distance\"=conf_matrix_distance),\n    roc_plot = p\n  ))\n} \n```\n:::\n\n\n\n## A real example\n\nApply the Naive Bayes classifier to the adult dataset for classification.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(ggplot2)\ndata <- read.csv(\"adult.data\",\n                 na.strings = \" ?\", header=FALSE)\nnewdata <- read.table(\"adult.test\",\n                      sep=\",\", skip = 1,na.strings = \" ?\")\nnewdata$V15 <- gsub(\"\\\\.$\", \"\", newdata$V15)\nsystem.time({\n  model <- naiveBayes(data,\"V15\",alpha = 1)\n  pred <- predict(model,newdata,\"raw\")  \n  roc <- roc_auc(newdata$V15,pred,\" <=50K\")\n})\n#>    user  system elapsed \n#>   1.763   0.079   1.844\nroc$optimal_tresholds\n#>   youden distance \n#>    0.985    0.971\nroc$confusion_matrix\n#> $youden\n#>           Predicted\n#> Actual     Positive Negative\n#>   Positive     9278     3157\n#>   Negative      470     3376\n#> \n#> $distance\n#>           Predicted\n#> Actual     Positive Negative\n#>   Positive     9832     2603\n#>   Negative      657     3189\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nroc\n#> $auc\n#> [1] 0.8901925\n#> \n#> $optimal_tresholds\n#>   youden distance \n#>    0.985    0.971 \n#> \n#> $confusion_matrix\n#> $confusion_matrix$youden\n#>           Predicted\n#> Actual     Positive Negative\n#>   Positive     9278     3157\n#>   Negative      470     3376\n#> \n#> $confusion_matrix$distance\n#>           Predicted\n#> Actual     Positive Negative\n#>   Positive     9832     2603\n#>   Negative      657     3189\n#> \n#> \n#> $roc_plot\n```\n\n::: {.cell-output-display}\n![](Naive-bayes-model_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nroc$roc_plot\n```\n\n::: {.cell-output-display}\n![](Naive-bayes-model_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n:::\n\n\n\n\nThis function prints a summary of the fitted model, including the prior probabilities, the conditional probabilities for each variable, and an indication of whether each variable is numeric or categorical.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint.naiveBayes <- function(model) {\n  cat(\"Naive Bayes Model\\n\")\n  cat(\"=================\\n\")\n  \n  # Print class levels\n  cat(\"\\nClass Levels:\\n\")\n  print(model$level)\n  \n  # Print prior probabilities\n  cat(\"\\nPrior Probabilities:\\n\")\n  print(model$prior)\n  \n  # Loop through the model tables to display statistics for each feature\n  cat(\"\\nFeature Statistics:\\n\")\n  for (i in seq_along(model$tables)) {\n    feature_name <- names(model$tables)[i]\n    cat(\"\\nFeature:\", feature_name, \"\\n\")\n    \n    if (model$isnumeric[i]) {\n      # For numeric features, print mean and standard deviation\n      cat(\"Type: Numeric\\n\")\n      stats <- model$tables[[i]]\n      print(data.frame(\n        Mean = stats[, 1], SD = stats[, 2]))\n    } else {\n      # For categorical features, print conditional probabilities\n      cat(\"Type: Categorical\\n\")\n      probs <- model$tables[[i]]\n      print(as.data.frame.matrix(probs))\n    }\n  }\n  \n  cat(\"\\n=================\\n\")\n}\n\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(model)\n#> Naive Bayes Model\n#> =================\n#> \n#> Class Levels:\n#> [1] \" <=50K\" \" >50K\" \n#> \n#> Prior Probabilities:\n#> label\n#>     <=50K      >50K \n#> 0.7591904 0.2408096 \n#> \n#> Feature Statistics:\n#> \n#> Feature: V1 \n#> Type: Numeric\n#>            Mean       SD\n#>  <=50K 36.78374 14.02009\n#>  >50K  44.24984 10.51903\n#> \n#> Feature: V2 \n#> Type: Categorical\n#>         Federal-gov  Local-gov  Never-worked  Private  Self-emp-inc\n#>  <=50K   0.02555994 0.06398648  0.0003465754 0.768271    0.02144435\n#>  >50K    0.04857665 0.08069992  0.0001305824 0.648211    0.08135283\n#>         Self-emp-not-inc  State-gov  Without-pay\n#>  <=50K        0.07875926 0.04098254 0.0006498289\n#>  >50K         0.09467224 0.04622617 0.0001305824\n#> \n#> Feature: V3 \n#> Type: Numeric\n#>            Mean       SD\n#>  <=50K 190340.9 106482.3\n#>  >50K  188005.0 102541.8\n#> \n#> Feature: V4 \n#> Type: Categorical\n#>               10th        11th        12th      1st-4th     5th-6th     7th-8th\n#>  <=50K 0.035252264 0.045116429 0.016211190 0.0065895860 0.012855757 0.024539133\n#>  >50K  0.008018328 0.007763778 0.004327351 0.0008909253 0.002163676 0.005218277\n#>                9th  Assoc-acdm  Assoc-voc  Bachelors   Doctorate   HS-grad\n#>  <=50K 0.019728331  0.03246281 0.04131630  0.1267384 0.004366106 0.3568483\n#>  >50K  0.003563701  0.03385516 0.04607356  0.2828051 0.039073438 0.2133130\n#>           Masters   Preschool  Prof-school  Some-college\n#>  <=50K 0.03092658 0.002102199  0.006225744     0.2387209\n#>  >50K  0.12218404 0.000127275  0.053964618     0.1766578\n#> \n#> Feature: V5 \n#> Type: Numeric\n#>             Mean       SD\n#>  <=50K  9.595065 2.436147\n#>  >50K  11.611657 2.385129\n#> \n#> Feature: V6 \n#> Type: Categorical\n#>          Divorced  Married-AF-spouse  Married-civ-spouse  Married-spouse-absent\n#>  <=50K 0.16099810       0.0005661827           0.3350588            0.015570025\n#>  >50K  0.05912334       0.0014016310           0.8528287            0.004459735\n#>         Never-married   Separated    Widowed\n#>  <=50K     0.41222146 0.038823958 0.03676143\n#>  >50K      0.06269113 0.008537207 0.01095821\n#> \n#> Feature: V7 \n#> Type: Categorical\n#>         Adm-clerical  Armed-Forces  Craft-repair  Exec-managerial\n#>  <=50K    0.14140889  0.0003899142     0.1373798       0.09093666\n#>  >50K     0.06628392  0.0002609603     0.1213466       0.25691545\n#>         Farming-fishing  Handlers-cleaners  Machine-op-inspct  Other-service\n#>  <=50K       0.03812495         0.05567109         0.07594663     0.13685989\n#>  >50K        0.01513570         0.01135177         0.03275052     0.01800626\n#>         Priv-house-serv  Prof-specialty  Protective-serv     Sales\n#>  <=50K     0.0064552465      0.09886492       0.01901915 0.1155879\n#>  >50K      0.0002609603      0.24269311       0.02766180 0.1283925\n#>         Tech-support  Transport-moving\n#>  <=50K    0.02798718        0.05536782\n#>  >50K     0.03705637        0.04188413\n#> \n#> Feature: V8 \n#> Type: Categorical\n#>          Husband  Not-in-family  Other-relative   Own-child  Unmarried\n#>  <=50K 0.2942651      0.3013023     0.038218879 0.202297177 0.13059128\n#>  >50K  0.7543010      0.1092137     0.004842615 0.008665732 0.02790875\n#>              Wife\n#>  <=50K 0.03332524\n#>  >50K  0.09506818\n#> \n#> Feature: V9 \n#> Type: Categorical\n#>         Amer-Indian-Eskimo  Asian-Pac-Islander      Black       Other     White\n#>  <=50K         0.011162791          0.03089990 0.11073812 0.009989889 0.8372093\n#>  >50K          0.004715779          0.03530461 0.04945195 0.003313790 0.9072139\n#> \n#> Feature: V10 \n#> Type: Categorical\n#>           Female      Male\n#>  <=50K 0.3880349 0.6119651\n#>  >50K  0.1504526 0.8495474\n#> \n#> Feature: V11 \n#> Type: Numeric\n#>             Mean         SD\n#>  <=50K  148.7525   963.1393\n#>  >50K  4006.1425 14570.3790\n#> \n#> Feature: V12 \n#> Type: Numeric\n#>             Mean       SD\n#>  <=50K  53.14292 310.7558\n#>  >50K  195.00153 595.4876\n#> \n#> Feature: V13 \n#> Type: Numeric\n#>            Mean       SD\n#>  <=50K 38.84021 12.31899\n#>  >50K  45.47303 11.01297\n#> \n#> Feature: V14 \n#> Type: Categorical\n#>            Cambodia      Canada       China     Columbia        Cuba\n#>  <=50K 0.0005344516 0.003412268 0.002302253 0.0023844762 0.002918928\n#>  >50K  0.0010341262 0.005170631 0.002714581 0.0003877973 0.003360910\n#>         Dominican-Republic      Ecuador  El-Salvador     England       France\n#>  <=50K        0.0028367045 0.0010277915  0.004028943 0.002507811 0.0007400099\n#>  >50K         0.0003877973 0.0006463289  0.001292658 0.004007239 0.0016804550\n#>            Germany       Greece    Guatemala        Haiti  Holand-Netherlands\n#>  <=50K 0.003864496 0.0009044565 0.0025489229 0.0016855780        8.222332e-05\n#>  >50K  0.005816960 0.0011633919 0.0005170631 0.0006463289        1.292658e-04\n#>            Honduras         Hong      Hungary       India        Iran\n#>  <=50K 0.0005344516 0.0006166749 0.0004522283 0.002507811 0.001068903\n#>  >50K  0.0002585315 0.0009048604 0.0005170631 0.005299897 0.002456050\n#>             Ireland       Italy     Jamaica       Japan         Laos\n#>  <=50K 0.0008222332 0.002014471 0.002960039 0.001603355 0.0006988982\n#>  >50K  0.0007755946 0.003360910 0.001421923 0.003231644 0.0003877973\n#>             Mexico    Nicaragua  Outlying-US(Guam-USVI-etc)         Peru\n#>  <=50K 0.025119224 0.0013566848                0.0006166749 0.0012333498\n#>  >50K  0.004395036 0.0003877973                0.0001292658 0.0003877973\n#>         Philippines      Poland     Portugal  Puerto-Rico     Scotland\n#>  <=50K  0.005673409 0.002014471 0.0013977964  0.004234501 0.0004111166\n#>  >50K   0.008014478 0.001680455 0.0006463289  0.001680455 0.0005170631\n#>              South      Taiwan     Thailand  Trinadad&Tobago  United-States\n#>  <=50K 0.002672258 0.001315573 0.0006577865     0.0007400099      0.9044565\n#>  >50K  0.002197518 0.002714581 0.0005170631     0.0003877973      0.9270941\n#>             Vietnam   Yugoslavia\n#>  <=50K 0.0025900345 0.0004522283\n#>  >50K  0.0007755946 0.0009048604\n#> \n#> =================\n```\n:::",
    "supporting": [
      "Naive-bayes-model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}