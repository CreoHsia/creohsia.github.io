{
  "hash": "a0f98bd1b607dce068b82f5d21bb263c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Generative learning algorithms\"\nauthor: \"Creo Hsia\"\ndate: \"2024-11-14\"\ndescription: \"Generative learning algorithms study record\"\ncategories: [Statistics Estimation, Optimization, MLE]\nformat: \n  html:\n    highlight-style: ayu\n    code-fold: true\n    code-copy: true \n    code-overflow: 'scroll'\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    editor: visual\nknitr:\n  opts_chunk: \n    fig.path: \"image/\"\n    collapse: true\n    comment: \"#>|\"\n---\n\n\n\n\n\n\nThere are two philosophies in classification problems:\n\n1. **Decision Boundary Approach (e.g., Logistic Regression, Perceptron Algorithm)**:\n   + This approach focuses on finding a decision boundary, typically a straight line, that separates two classes (e.g., elephants and dogs) in the feature space.\n   + A model is trained on a given dataset, and once the boundary is established, a new input is classified by checking which side of the boundary it falls on.\n   + The decision-making process involves a comparison of the position of the new data point relative to the learned boundary.\n\n2. **Model-Based Approach**:\n   + Instead of finding a direct boundary, this approach builds separate models for each class (e.g., a model of what elephants look like and a model of what dogs look like).\n   + When classifying a new input, the model compares how closely the input resembles each class model.\n   + The final classification is based on which model the new input matches more closely, indicating which class it is most similar to.\n\n> In essence, while the first approach relies on establishing a single boundary to differentiate between classes, the second approach builds individual class models and uses similarity matching for classification.\n\nAlgorithms that aim to learn p(y|x) directly, like logistic regression, or those that map inputs X directly to labels (e.g., the perceptron algorithm), are known as **discriminative learning algorithms**. In contrast, the focus here is on algorithms that model p(x|y) and p(y) instead. These algorithms are called **generative learning algorithms**.\n\nAfter modeling $p(y)$ (called the **class priors**) and $p(x|y)$, we can then use the Bayes rule to derive the posterior distribution on $y$ given $x$\n$$\np(y \\mid x)=\\frac{p(x \\mid y) p(y)}{p(x)}\n$$\nThe denominator can be derived by $p(x)=p(x\\mid y=1)p(y=1)+p(x|y=0)p(y=0)$ and thus can also be expressed in terms of the quantities $p(x|y)$ and $p(y)$ that we've learned. **Actually, if we are calculating $p(y|x)$ in order to make a prediction we don't actually need to calculate the denominator.**\n\n## Gaussian discriminant analysis\n\n### The Gaussian discriminant analysis model\n\nWhen we have a classification problem in which the **input features $x$ are**\n**continuous-valued random variables**, we can then use the Gaussian Discriminant Analysis (GDA) model, which models $p(x|y)$ using a multivariate normal distribution.\n$$\n\\begin{aligned}\ny & \\sim \\operatorname{Bernoulli}(\\phi) \\\\\nx \\mid y=0 & \\sim \\mathcal{N}\\left(\\mu_0, \\Sigma\\right) \\\\\nx \\mid y=1 & \\sim \\mathcal{N}\\left(\\mu_1, \\Sigma\\right)\n\\end{aligned}\n$$\nThe log-likelihood of the data is given by\n$$\n\\begin{aligned}\n\\ell\\left(\\phi, \\mu_0, \\mu_1, \\Sigma\\right) & =\\log \\prod_{i=1}^n p\\left(x^{(i)}, y^{(i)} ; \\phi, \\mu_0, \\mu_1, \\Sigma\\right) \\\\\n& =\\log \\prod_{i=1}^n p\\left(x^{(i)} \\mid y^{(i)} ; \\mu_0, \\mu_1, \\Sigma\\right) p\\left(y^{(i)} ; \\phi\\right) .\n\\end{aligned}\n$$\nBy maximizing $\\ell$ with respect to the parameters, we find the maximum likelihood estimate of the parameters  to be:\n$$\n\\begin{aligned}\n\\phi & =\\frac{1}{n} \\sum_{i=1}^n 1\\left\\{y^{(i)}=1\\right\\} \\\\\n\\mu_0 & =\\frac{\\sum_{i=1}^n 1\\left\\{y^{(i)}=0\\right\\} x^{(i)}}{\\sum_{i=1}^n 1\\left\\{y^{(i)}=0\\right\\}} \\\\\n\\mu_1 & =\\frac{\\sum_{i=1}^n 1\\left\\{y^{(i)}=1\\right\\} x^{(i)}}{\\sum_{i=1}^n 1\\left\\{y^{(i)}=1\\right\\}} \\\\\n\\Sigma & =\\frac{1}{n} \\sum_{i=1}^n\\left(x^{(i)}-\\mu_{y^{(i)}}\\right)\\left(x^{(i)}-\\mu_{y^{(i)}}\\right)^T\n\\end{aligned}\n$$\nThen we have\n\n\\begin{aligned}\np(x|y=1)p(y)=\\phi_1\\frac{1}{(2 \\pi)^{d / 2}|\\Sigma|^{1 / 2}} \\exp \\left(-\\frac{1}{2}(x-\\mu_1)^T \\Sigma^{-1}(x-\\mu_1)\\right) .\\\\\n\np(x|y=0)p(y)=\\phi_0\\frac{1}{(2 \\pi)^{d / 2}|\\Sigma|^{1 / 2}} \\exp \\left(-\\frac{1}{2}(x-\\mu_0)^T \\Sigma^{-1}(x-\\mu_0)\\right) .\\\\\n\\end{aligned}\n\nand hence\n\n\\begin{aligned}\n\\log p(x|y=1)p(y)=&\\log \\phi_1-\\frac{1}{2}(x-\\mu_1)^T \\Sigma^{-1}(x-\\mu_1)+C\\\\\n\\log p(x|y=0)p(y)=&\\log \\phi_0-\\frac{1}{2}(x-\\mu_0)^T \\Sigma^{-1}(x-\\mu_0)+C\\\\\n\n\\end{aligned}\nwe assign $x$ to $y=1$ if\n$$\n\\log p(x|y=1)p(y)-\\log p(x|y=0)p(y)>0\n$$\nThat is \n$$\n\\begin{aligned}\n&\\log(\\phi_1 /\\phi_0)-\\frac{1}{2}\\left((x-\\mu_1)^T \\Sigma^{-1}(x-\\mu_1)-(x-\\mu_0)^T \\Sigma^{-1}(x-\\mu_0) \\right)\\\\\n=&\\log(\\phi_1 /\\phi_0)-\\frac{1}{2}\\left(x-\\mu_1+x-\\mu_0 \\right)^T\\Sigma^{-1} \\left(x-\\mu_1-x+\\mu_0 \\right)\\\\\n=&\\log(\\phi_1 /\\phi_0)-\\left(x-\\frac{\\mu_1+\\mu_0}{2} \\right)^T\\Sigma^{-1} \\left(\\mu_1-\\mu_0 \\right)\n\\end{aligned}\n$$\nIt is a linear function of $x$. (We can show that if the covariance matrix are not equal, then it is quadratic function.)\n\n### Discussion: GDA and logistic regression\n\nIf we view the quantity $p\\left(y=1 \\mid x ; \\phi, \\mu_0, \\mu_1, \\Sigma\\right) \\text { as a function of } x$ as a function of $x$, we'll find that it can be expressed in the form:\n$$\n\\begin{aligned}\np(y=1 \\mid x)=&\\frac{p(x \\mid y=1) p(y=1)}{p(x\\mid y=1)p(y=1)+p(x|y=0)p(y=0)}\\\\\n=&\\frac{1}{1+\\frac{p(x|y=0)p(y=0)}{p(x\\mid y=1)p(y=1)}}\\\\\n=&\\frac{1}{1+\\exp \\left(-\\theta^T x\\right)}\n\\end{aligned}\n$$\nwhere $\\theta$ is some appropriate function of $\\phi, \\Sigma, \\mu_0, \\mu_1$. (here we add $x_0=1$).\n\n> GDA and logistic regression will, in general, give different decision boundaries when trained on the same dataset. Which is better?\n\nWe just argued that if $p(x \\mid y)$ is multivariate gaussian (with shared $\\Sigma$ ), then $p(y \\mid x)$ necessarily follows a logistic function. **The converse, however, is not true**; i.e., $p(y \\mid x)$ being a logistic function does not imply $p(x \\mid y)$ is multivariate gaussian. \n\nThis shows that **GDA makes stronger modeling assumptions** about the data than does logistic regression.  It turns out that when **these modeling assumptions are correct**, then GDA will find better fits to the data, and is a better model.\n\nIn contrast, by making significantly weaker assumptions, logistic regression is also more **robust** and **less sensitive** to incorrect modeling assumptions. \n\n## Naive bayes\n\nFor our motivating example, consider building an email spam filter using machine learning. Classifying emails is one example of a broader set of problems called **text classification.**\n\nLet's say we have a training set (a set of emails labeled as spam or non-spam). We first specify the features $x_j$ used to represent an email.\n\nWe will represent an email via a **feature vector** whose length is equal to the number of words in the dictionary. Specifically, if an email contains the $j$-th word of the dictionary, then we will set $x_j = 1$, otherwise, we let $x_j = 0$.\n\n> Instead of using all English words, it is common practice to **use only words found in the training set** as features to **reduce computational and space requirements**. This method also **allows for the inclusion of unique words not found in dictionaries**. Additionally, very high-frequency words (stop words) like “the,” “of,” and “and” are often excluded, as they appear in many documents and **provide little value in distinguishing spam from non-spam emails**.\n\nThe set of words encoded into the feature vector is called the ***vocabulary***, so the dimension of x is equal to the size of the vocabulary.\n\nTo model $p(x|y)$, we will therefore make a very strong assumption. We will assume that the $x_i$'s are **conditionally independent given $y$.**  This assumption is called the ***Naive Bayes (NB) assumption,*** and the resulting algorithm is called the ***Naive Bayes classifier***.\n\n> We don't use multinomial distribution, since the dimension of vocabulary is high.  If there are 5000 words, we will have $2^{5000}-1$ parameters.\n\n> The independence is assumed given $y$. This does not means that they are independent.\n\nWe know have \n$$\n\\begin{aligned}\n& p\\left(x_1, \\ldots, x_{d} \\mid y\\right) \\\\\n& \\quad=p\\left(x_1 \\mid y\\right) p\\left(x_2 \\mid y, x_1\\right) p\\left(x_3 \\mid y, x_1, x_2\\right) \\cdots p\\left(x_{d} \\mid y, x_1, \\ldots, x_{d-1}\\right) \\\\\n& \\quad=p\\left(x_1 \\mid y\\right) p\\left(x_2 \\mid y\\right) p\\left(x_3 \\mid y\\right) \\cdots p\\left(x_{d} \\mid y\\right) \\\\\n& \\quad=\\prod_{j=1}^d p\\left(x_j \\mid y\\right)\n\\end{aligned}\n$$\nOur model is parameterized by $\\phi_{j \\mid y=1}=p\\left(x_j=1 \\mid y=1\\right), \\phi_{j \\mid y=0}=p\\left(x_j=\\right.$ $1 \\mid y=0)$, and $\\phi_y=p(y=1)$. As usual, given a training set $\\left\\{\\left(x^{(i)}, y^{(i)}\\right) ; i=\\right.$ $1, \\ldots, n\\}$, we can write down the joint likelihood of the data:\n$$\n\\mathcal{L}\\left(\\phi_y, \\phi_{j \\mid y=0}, \\phi_{j \\mid y=1}\\right)=\\prod_{i=1}^n p\\left(x^{(i)}, y^{(i)}\\right)\n$$\nMaximizing this with respect to $\\phi_y, \\phi_{j \\mid y=0}$ and $\\phi_{j \\mid y=1}$ gives the maximum likelihood estimates:\n\n$$\n\\begin{aligned}\n\\phi_{j \\mid y=1} & =\\frac{\\sum_{i=1}^n 1\\left\\{x_j^{(i)}=1 \\wedge y^{(i)}=1\\right\\}}{\\sum_{i=1}^n 1\\left\\{y^{(i)}=1\\right\\}} \\\\\n\\phi_{j \\mid y=0} & =\\frac{\\sum_{i=1}^n 1\\left\\{x_j^{(i)}=1 \\wedge y^{(i)}=0\\right\\}}{\\sum_{i=1}^n 1\\left\\{y^{(i)}=0\\right\\}} \\\\\n\\phi_y & =\\frac{\\sum_{i=1}^n 1\\left\\{y^{(i)}=1\\right\\}}{n}\n\\end{aligned}\n$$\n\nIn the equations above, the \" $\\wedge$ \" symbol means \"and.\"  The parameters have\na very natural interpretation.\n\nHaving fit all these parameters, to make a prediction on a new example with features $x$, we then simply calculate\n$$\n\\begin{aligned}\np(y=1 \\mid x) & =\\frac{p(x \\mid y=1) p(y=1)}{p(x)} \\\\\n& =\\frac{\\left(\\prod_{j=1}^d p\\left(x_j \\mid y=1\\right)\\right) p(y=1)}{\\left(\\prod_{j=1}^d p\\left(x_j \\mid y=1\\right)\\right) p(y=1)+\\left(\\prod_{j=1}^d p\\left(x_j \\mid y=0\\right)\\right) p(y=0)}\n\\end{aligned}\n$$\nThe generalization to $x_j$ takes more than 2 values is straightforward. \n\n### Laplace smoothing\n\nIt is statistically a bad idea to estimate the probability of some event to be zero just because you haven't seen it before in your **finite** training set. \n\n Take the problem of estimating the mean of a multinomial random variable z taking values in $\\{1, \\ldots, k\\}$. We can pa-rameterize our multinomial with $\\phi_j=p(z=j)$. Given a set of $n$ independent observations the maximum likelihood estimates are given by\n$$\n\\phi_j=\\frac{\\sum_{i=1}^n 1\\left\\{z^{(i)}=j\\right\\}}{n} .\n$$\nTo avoid that  some of the $\\phi_j$'s  might end up as zero, we can use ***Laplace smoothing***\n$$\n\\phi_j=\\frac{1+\\sum_{i=1}^n 1\\left\\{z^{(i)}=j\\right\\}}{k+n} .\n$$\nHere, we've added $1$ to the numerator, and $k$ to the denominator. ( $\\sum_{i=1}^k\\phi_i=1$ still holds). \n\n### Event models for text classification\n\nWhile Naive Bayes as we've presented it will work well for many classification problems, for text classification, there is a related model that does even better.\n\nTo describe this model, we will use a different notation and set of features for representing emails.\n\n+ Each word in an email is represented by $x_j$ , where $j$ denotes the position of the word within the email.\n+  $x_j$ is an integer that corresponds to the identity of the word in the vocabulary, which means each word in the vocabulary is assigned a unique number from 1 to $|V|$ (where $|V|$ is the total number of unique words in the vocabulary).\n\nWe assume \n\n+ the way an email is generated is via a random process in which **spam/non-spam is first determined** (according to p(y)).\n+  the sender of the email writes the email by first generating $x_1$ from some multinomial distribution over words $(p(x_1|y))$.\n+  the second word $x_2$ is chosen **independently** of $x_1$ but from the **same multinomial distribution**\n+ and similarly for $x_3$, $x_4 \\ldots$\n\nThus, the overall probability of a message is given by\n$$\np(y) \\prod_{j=1}^d p\\left(x_j \\mid y\\right)\n$$\nNow, $x_j|y$ is a multinomial, rather than a Bernoulli distribution.\n\nThe parameters for our new model are $\\phi_y=p(y)$ , $\\phi_{k \\mid y=1}=$ $p\\left(x_j=k \\mid y=1\\right)($ for any $j)$ and $\\phi_{k \\mid y=0}=p\\left(x_j=k \\mid y=0\\right)$. Note that we have assumed that $p\\left(x_j \\mid y\\right)$ is the same for all values of $j$ (i.e., that the distribution according to which a word is generated d**oes not depend on its position $j$** ).\n\nIf we are given a training set, the likelihood of the data is given by\n$$\n\\begin{aligned}\n\\mathcal{L}\\left(\\phi_y, \\phi_{k \\mid y=0}, \\phi_{k \\mid y=1}\\right) & =\\prod_{i=1}^n p\\left(x^{(i)}, y^{(i)}\\right) \\\\\n& =\\prod_{i=1}^n\\left(\\prod_{j=1}^{d_i} p\\left(x_j^{(i)} \\mid y ; \\phi_{k \\mid y=0}, \\phi_{k \\mid y=1}\\right)\\right) p\\left(y^{(i)} ; \\phi_y\\right) .\n\\end{aligned}\n$$\nMaximizing this yields the maximum likelihood estimates of the parameters:\n$$\n\\begin{aligned}\n\\phi_{k \\mid y=1} & =\\frac{\\sum_{i=1}^n \\sum_{j=1}^{d_i} 1\\left\\{x_j^{(i)}=k \\wedge y^{(i)}=1\\right\\}}{\\sum_{i=1}^n 1\\left\\{y^{(i)}=1\\right\\} d_i} \\\\\n\\phi_{k \\mid y=0} & =\\frac{\\sum_{i=1}^n \\sum_{j=1}^{d_i} 1\\left\\{x_j^{(i)}=k \\wedge y^{(i)}=0\\right\\}}{\\sum_{i=1}^n 1\\left\\{y^{(i)}=0\\right\\} d_i} \\\\\n\\phi_y & =\\frac{\\sum_{i=1}^n 1\\left\\{y^{(i)}=1\\right\\}}{n} .\n\\end{aligned}\n$$\nIf we were to apply Laplace smoothing (which is needed in practice for good performance), we obtain:\n$$\n\\begin{aligned}\n& \\phi_{k \\mid y=1}=\\frac{1+\\sum_{i=1}^n \\sum_{j=1}^{d_i} 1\\left\\{x_j^{(i)}=k \\wedge y^{(i)}=1\\right\\}}{|V|+\\sum_{i=1}^n 1\\left\\{y^{(i)}=1\\right\\} d_i} \\\\\n& \\phi_{k \\mid y=0}=\\frac{1+\\sum_{i=1}^n \\sum_{j=1}^{d_i} 1\\left\\{x_j^{(i)}=k \\wedge y^{(i)}=0\\right\\}}{|V|+\\sum_{i=1}^n 1\\left\\{y^{(i)}=0\\right\\} d_i}\n\\end{aligned}\n$$",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}