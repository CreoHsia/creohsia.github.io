{
  "hash": "a3ac8f55312c1a331391b065b92b479d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Support Vector Machines\"\nauthor: \"Creo Hsia\"\ndate: \"2024-11-14\"\ndescription: \"SVM study record\"\ncategories: [Statistics Estimation, Optimization, MLE]\nformat: \n  html:\n    highlight-style: ayu\n    code-fold: true\n    code-copy: true \n    code-overflow: 'scroll'\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    editor: visual\nknitr:\n  opts_chunk: \n    fig.path: \"image/\"\n    collapse: true\n    comment: \"#>|\"\n---\n\n\n\n\n\n\n## Margins\n\nConsider our logistic regression model, where the probability $p(y=1|x;\\theta)$ is modeled by $g(\\theta^T x)$. We predict \"1\" on an input $x$ iff $\\theta^Tx\\geq0$. That means **the larger $\\theta^T x$ is, the larger also is $p(y=1|x;w,b)$.** \n\nThus, informally, we can think of our prediction as being a very confident one that $y=1$ if $\\theta^T x >>0$. Similarly, we think of logistic regression as making a very confident prediction of $y=0$, if $\\theta^Tx<<0$.\n\nAgain, informally, it seems that we'd found a good fit for the training data if we can find $\\theta$ so that $\\theta x^{(i)}>>0$ whenever $y^{(i)}=1$, and $\\theta^T x^{(i)}<<0$ whenever $y^{(i)}=0$, since this would **reflect a very confident (and correct) set of classifications for all the training examples**. \n\n> Again, informally, it'd be nice if, given a training set, we could find a **decision boundary** that allows us to **make all correct and confident (meaning far from the decision boundary) predictions on the training examples.**\n\n## Notation\n\nWe will consider a **linear classifier** for a **binary classification** problem with labels $y$ and features $x$.\n\nFrom now on, we'll use $y \\in \\{−1, 1\\}$ (instead of $\\{0, 1\\}$) to denote the class labels. We will use parameters $w,b$ and write our classifier as\n$$\nh_{w, b}(x)=g\\left(w^T x+b\\right) .\n$$\nHere, $g(z)=1$ if $z \\geq 0$, and $g(z)=-1$ otherwise.\n\n>  This \"$w,b$\" notation allows us to explicitly **treat the intercept term** $b$ **separately** from the other parameters.\n\n> Our classifier will **directly predict either $1$ or $−1$** , without first going through the intermediate step of estimating the probability of $y$ being $1$ (which was what logistic regression did).\n\n## Functional and Geometric Margins\n\n### Functional Margins\n\n**Functional Margin Definition:**\n\nFor a training example $(x^{(i)}, y^{(i)})$, the **functional margin** with respect to a set of parameters $(w, b)$ is defined as:\n$$\n\\hat{\\gamma}^{(i)}=y^{(i)}\\left(w^T x^{(i)}+b\\right)\n$$\n\n> This value essentially measures **how confident the model is** in predicting that a given training example is correctly classified.\n\n**Interpretation of Functional Margin**:\n\n+ If $y^{(i)} = 1$ , for the functional margin to be large (indicating confidence in the prediction), $w^T x^{(i)} + b$ needs to be a large positive number.\n+ If $y^{(i)} = -1$ , for the functional margin to be large, $w^T x^{(i)} + b$ should be a large negative number.\n+ If the product $y^{(i)} (w^T x^{(i)} + b) > 0$ , it means that the model's prediction is correct\n\n> + A large functional margin indicates that the model is confidently and correctly classifying the example.\n> + It helps in assessing not just **whether the model makes a correct prediction**, but also **how confident it is about that prediction**.\n\n***<u>There's one property of the functional margin that makes it not a very good measure of confidence:</u>***\n\n+ The classifier function $g$ (used in predicting $\\pm 1$ ) **depends on the sign** of $w^T x + b$ but **not on its magnitude**.\n+ **scaling** $w$ and $b$ (e.g., multiplying them by $2$) does **not change the decision boundary**, as the sign of $w^T x + b$ remains the same.\n+ Hence, we can make the functional margin as large as we want **without changing the underlying classification.** (if we scale the parameter, also the functional margin will be scaled) Therefore, the functional margin is **not a reliable measure of confidence** on its own.\n\n***<u>Solution: Normalization</u>***\n\n+ By **normalizing** $w$ and $b$ (i.e., considering $(w / ||w||_2, b / ||w||_2)$ ), the functional margin becomes a **more meaningful** measure because it restricts $w$ and $b$ from being scaled arbitrarily.\n\nGiven a training set $S=\\left\\{\\left(x^{(i)}, y^{(i)}\\right) ; i=1, \\ldots, m\\right\\}$ we also define the function margin of ($w,b$) with respect to $S$ to be the **smallest of the functional margins** of the individual training examples, denoted by $\\hat{\\gamma}$.\n$$\n\\hat{\\gamma}=\\min _{i=1, \\ldots, m} \\hat{\\gamma}^{(i)}\n$$\n\n### Geometric Margins\n\n![Geomeric margrgin](geometric margin.png)\n\nThe decision boundary corresponding to $(w,b)$ is shown, along with the vector $w$. Note that $w$ is **orthogonal to the separating hyperplane**. \n\nConsider the point at $A$. Its distance to the decision boundary, $\\gamma(i)$, is given by the line segment $AB$.\n\n$w /\\|w\\|$ is a unit-length vector pointing in the same direction as $w$. Since $A$ represents $x^{(i)}$, we therefore find that point $B$ is given by $x^{(i)}-\\gamma^{(i)} \\cdot w /\\|w\\|$. Since $B$ is on the boundary so $B$ satisfy the equation $w^Tx+b=0$. Hence,\n$$\nw^T\\left(x^{(i)}-\\gamma^{(i)} \\frac{w}{\\|w\\|}\\right)+b=0\n$$\nSolving for $\\gamma(i)$ yields\n$$\n\\gamma^{(i)}=\\frac{w^T x^{(i)}+b}{\\|w\\|}=\\left(\\frac{w}{\\|w\\|}\\right)^T x^{(i)}+\\frac{b}{\\|w\\|} .\n$$\nMore generally, we define the **geometric margin** of $(w, b)$ with respect to a training example $\\left(x^{(i)}, y^{(i)}\\right)$ to be\n\n$$\n\\gamma^{(i)}=y^{(i)}\\left(\\left(\\frac{w}{\\|w\\|}\\right)^T x^{(i)}+\\frac{b}{\\|w\\|}\\right) .\n$$\n\n+ $\\gamma(i)>0$ if the model's prediction is correct\n+ if $\\|w\\|=1$, then the **functional margin equals the geometric margin**\n+ the geometric margin is **invariant to rescaling** of the parameters\n+ This invariance allows us to impose any **arbitrary scaling constraint** on $w$ without affecting the model's performance or decision boundary. When fitting w and b to training data, we can choose constraints such as:\n  +  $||w|| = 1$ (normalizing $w$ to have a unit norm)\n  +  $|w_1|$ = 5 \n  +  $|w_1 + b| + |w_2| = 2$\n+ These constraints can be satisfied by **rescaling** $w$ and $b$ .\n\nFinally, given a training set $S=\\left\\{\\left(x^{(i)}, y^{(i)}\\right) ; i=1, \\ldots, m\\right\\}$, we also define the geometric margin of $(w, b)$ with respect to $S$ to be the **smallest of the geometric margins** on the individual training examples:\n\n$$\n\\gamma=\\min _{i=1, \\ldots, m} \\gamma^{(i)}\n$$\n\n## The Optimal Margin Classifier\n\nFrom our previous discussion, it is natural to find a decision boundary that **maximizes the (geometric) margin**, since this would reflect a very **confident set of predictions** on the training set and a **good \"fit\" to the training data**. \n\nSpecifically, this will result in a classifier that separates the positive and the negative training examples with a \"gap\" (geometric margin).\n\nWe  assume that we are given a training set that is **linearly separable**; i.e., that it is possible to **separate** the positive and negative examples **using some separating hyperplane.**\n\nHow we we find the one that achieves the maximum geometric margin? We can pose the following optimization problem:\n$$\n\\begin{aligned}\n\\max _{\\gamma, w, b} & \\gamma \\\\\n\\text { s.t. } & y^{(i)}\\left(w^T x^{(i)}+b\\right) \\geq \\gamma, \\quad i=1, \\ldots, m \\\\\n& \\|w\\|=1\n\\end{aligned}\n$$\nwe want to maximize $\\gamma$, subject to each training example having geometric margin at least $\\gamma$.\n\n> The $||w|| = 1$ constraint moreover ensures that the functional margin equals to the geometric margin\n\nBut the $||w|| = 1$ constraint is a nasty (non-convex) one. We transform the problem into: \n$$\n\\begin{aligned}\n\\max _{\\gamma, w, b} & \\frac{\\hat{\\gamma}}{\\|w\\|} \\\\\n\\text { s.t. } & y^{(i)}\\left(w^T x^{(i)}+b\\right) \\geq \\hat{\\gamma}, \\quad i=1, \\ldots, m\n\\end{aligned}\n$$\nHere $\\gamma=\\hat{\\gamma} /||w|$ using the relation of geometric and functional margins. Recall that we can add an arbitrary **scaling constraint** on $w $ and $b$ without changing anything (of geometic margins ). We can let $\\hat{\\gamma}=1$.\n\nPlugging this into our problem above and noting that maximizing $\\hat{\\gamma} /\\|w\\|=1 /\\|w\\|$ is the same thing as minimizing $\\|w\\|^2$, we now have the following optimization problem:\n\n$$\n\\begin{aligned}\n\\min _{\\gamma, w, b} & \\frac{1}{2}\\|w\\|^2 \\\\\n\\text { s.t. } & y^{(i)}\\left(w^T x^{(i)}+b\\right) \\geq 1, \\quad i=1, \\ldots, m\n\\end{aligned}\n$$\n\n## Lagrange Duality\n\nConsider a problem of the following form:",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}